-------------------------------------------------------------------------------
 ____      _       _                    _____             _      _____             
|    \ ___| |_ ___| |___ _ _ ___ ___   |   __|___ ___ ___| |_   |     |___ ___ ___ 
|  |  | .'|  _| .'| | .'| | | -_|  _|  |__   | . | .'|  _| '_|  |   --| . |  _| -_|
|____/|__,|_| |__,|_|__,|_  |___|_|    |_____|  _|__,|_| |_,_|  |_____|___|_| |___|
                        |___|                |_|                                   

 #datalayer-spark-core
-------------------------------------------------------------------------------
| EXAMPLE                                                                     |
-------------------------------------------------------------------------------
$ cd $SPARK_HOME
$ ./bin/run-example org.apache.spark.examples.SparkPi 2
-------------------------------------------------------------------------------
| SHELL                                                                       |
-------------------------------------------------------------------------------
+ word-count
-------------------------------------------------------------------------------
$ dly-hadoop-start
$ hdfs dfs -mkdir /word
$ hdfs dfs -ls /
$ hdfs dfs -put /dataset/gutenberg/pg20417.txt /word
$ hdfs dfs -ls /word
$ spark-shell
$ dly-spark-shell
scala> val file = sc.textFile("hdfs:///word/pg20417.txt")
scala> file.count()
scala> val errors = file.filter(line => line.contains("ERROR"))
// Count all the errors
scala> errors.count()
// Count errors mentioning MySQL
scala> errors.filter(line => line.contains("MySQL")).count()
// Fetch the MySQL errors as an array of strings
scala> errors.filter(line => line.contains("MySQL")).collect()
scala> errors.cache()
-------------------------------------------------------------------------------
+ pi-estimator
-------------------------------------------------------------------------------
$ spark-shell
$ dly-spark-shell
scala> val NUM_SAMPLES = 10000000000
scala> val count = sc.parallelize(1 to NUM_SAMPLES).map{i =>
  val x = Math.random()
  val y = Math.random()
  if (x*x + y*y < 1) 1 else 0
}.reduce(_ + _)
scala> println("Pi is roughly " + 4.0 * count / NUM_SAMPLES)
-------------------------------------------------------------------------------
+ logistic-regression
scala> val points = spark.textFile(...).map(parsePoint).cache()
scala> var w = Vector.random(D) // current separating plane
scala> for (i <- 1 to ITERATIONS) {
  val gradient = points.map(p =>
    (1 / (1 + exp(-p.y*(w dot p.x))) - 1) * p.y * p.x
  ).reduce(_ + _)
  w -= gradient
}
scala> println("Final separating plane: " + w)
-------------------------------------------------------------------------------
| SCALA                                                                       |
-------------------------------------------------------------------------------
$ dly-spark-scala -cp "$SPARK_HOME/lib/*" /i/spark/core/src/main/scala/SimpleSpark.scala localhost[1]
-------------------------------------------------------------------------------
